{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d1b7d9",
   "metadata": {},
   "source": [
    "Hello everyone,\n",
    "\n",
    "In this module, it is required to implement the feature extractor for Arabic Font Identification System.\n",
    "\n",
    "At first, what are the main steps that we should go through in this module?\n",
    "\n",
    "\n",
    "# TODOs:\n",
    "\n",
    "1. Understand the problem\n",
    "2. ..\n",
    "3. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbcf51",
   "metadata": {},
   "source": [
    "# 1. Understand the problem\n",
    "\n",
    "Calligraphers use some specific features, The main aim of this step is to transform these features into values that can be fed to a machine so it can decide on the style of a given text image. It should be mentioned that a feature might specify one or more styles. Features designated for each style or set of styles might be used in a sequential manner (sequential\n",
    "decision) or parallel manner.\n",
    "\n",
    "\n",
    "\n",
    "- Input: --\n",
    "- Output: --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b311bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### imports #####################################################\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from scipy import ndimage\n",
    "from skimage.feature import local_binary_pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb1f9a",
   "metadata": {},
   "source": [
    "# 2. Text thickness (Tth)\n",
    "Stroke thickness plays an important role in defining the style. Some styles use a flat pen, whereas some others use a pointed one. In some styles, calligraphers alter the thickness while writing (via pushing down the pen or the opposite), whereas in others, the thickness is always preserved. Modeling such a feature in form of a descriptor will help the machine to understand more specificities of each style. Text thickness (Tth) descriptor codifies the appearance frequency of different line thicknesses in a text image. To extract this descriptor, we employ both the skeleton and edge image, and thickness is determined by the distance between skeleton and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae7eccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tth(skeleton_img, edge_img, bins=5):\n",
    "    \"\"\"Gets normalized histogram of text thickness \n",
    "\n",
    "    Args:\n",
    "        skeleton_img: the image extracted skeleton\n",
    "        edge_img: the image extracted edges\n",
    "        \n",
    "    Returns:\n",
    "        hist : 1d array contains the normalized histogram values\n",
    "        bin_edges : 1d array contains the bin edges values\n",
    "    \"\"\"\n",
    "\n",
    "    skeleton = skeleton_img.copy()\n",
    "    edge = edge_img.copy()\n",
    "\n",
    "    # convert img to uint8 with 255:white 0:black\n",
    "\n",
    "    if skeleton.max() == 1:\n",
    "        skeleton = skeleton * 255\n",
    "        skeleton = skeleton.astype(np.uint8)\n",
    "\n",
    "    if edge.max() == 1:\n",
    "        edge = edge * 255\n",
    "        edge = edge.astype(np.uint8)\n",
    "\n",
    "    # flipping edge => black text on white background\n",
    "    edge = 255 - edge\n",
    "\n",
    "    distance = cv2.distanceTransform(edge+skeleton, distanceType=cv2.DIST_L2, maskSize=5)\n",
    "    # keeping only skeleton distances\n",
    "    distance[skeleton==0] = 0\n",
    "    \n",
    "    h, h_bins = np.histogram(distance, range=(1, distance.max()), density=False, bins=bins)\n",
    "\n",
    "    # normalizing the histogram\n",
    "    h_sum = h.sum()\n",
    "    if h_sum > 1:\n",
    "        h = h/h_sum\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47249f",
   "metadata": {},
   "source": [
    "# 3. Special Diacritics (SDs)\n",
    "Thuluth and Mohakik have a similar writing style that is decorated with diacritics having special shapes, as shown in Figure 9:\n",
    "<img src=\"sds.png\" alt=\"Figure 9\" width=\"100\"/>\n",
    "An SDs descriptor will be used to inspect the existence of such diacritics in a given text image. To this end, HuMoments is calculated for the contours of the two diacritics in Figure 9.\n",
    "Thereafter,  a score is calculated for a test image by calculating the distance between the two pre-calculated HuMoments and the test image diacritics contours HuMoments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "346b6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precalculate_hu(path=\"sds.png\", save=False, save_filename=\"sds_hue_moments\"):\n",
    "    \"\"\"Utility function used by SDs to calculate hu moments for each contour, from one image contains diactritics\n",
    "        \n",
    "    Args:\n",
    "        path: the path for the diactritics image\n",
    "        save: if true, it will store the calculated values in sds_hue_moments\n",
    "        save_filename: the filename in case of save=true, (default = sds_hue_moments)\n",
    "    Returns:\n",
    "        hu_moments: the calculated hu moments\n",
    "    \"\"\"\n",
    "\n",
    "    # importing binarize from preprocessing module\n",
    "    import sys\n",
    "    sys.path.insert(1, \"./../preprocessing/\")\n",
    "    from preprocessing import binarize\n",
    "\n",
    "    sds_img = binarize(cv2.imread(path, 0))\n",
    "    \n",
    "    contours, _ = cv2.findContours(image=sds_img.copy(), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    hu_moments = []\n",
    "    for contour in contours:\n",
    "        moments = cv2.moments(contour)\n",
    "        hu = cv2.HuMoments(moments)\n",
    "        hu = np.array(hu).flatten()\n",
    "        hu_moments.append(hu)\n",
    "\n",
    "    hu_moments = np.array(hu_moments)\n",
    "    \n",
    "    if save:\n",
    "        np.save(save_filename, hu_moments)\n",
    "\n",
    "    return hu_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c00606de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDs(img, recalculate=False, hu_file=\"sds_hue_moments.npy\", path=\"sds.png\"):\n",
    "    \"\"\"Gets an input image of the diacritics image and return a score determines how those diacritics are similar to mohakek and thuluth\n",
    "        \n",
    "    Args:\n",
    "        diacritics_image: A binary image containing only diacritics (white text on black bg)\n",
    "        recalculate: recalculate precalculate humoments values (default = False)\n",
    "        path: path of image to recalculate humoments from (default = \"sds.png\")\n",
    "        hu_file: file to read precalculated hu_moments from it or write to it in case of recalculate = True (default = \"sds_hue_moments.npy\")\n",
    "\n",
    "    Returns:\n",
    "        score: determine how those diacritics are similar to mohakek and thuluth (more similar => larger score)\n",
    "    \"\"\"\n",
    "    \n",
    "    # sd1 = np.array([2.40895486e-01, 4.84043539e-03, 1.38924440e-03, 3.48145819e-04, -6.47404757e-08, 6.90885170e-06, -2.33304273e-07])\n",
    "    # sd2 = np.array([3.54057491e-01, 3.05011527e-03, 3.44288395e-02, 7.77674538e-04, 3.14350358e-06, 4.13909592e-05, -2.51216325e-06])\n",
    "\n",
    "\n",
    "    if recalculate:\n",
    "        sd = precalculate_hu(path, True, hu_file)\n",
    "    else:\n",
    "        sd = np.load(hu_file)\n",
    "\n",
    "    contours, _ = cv2.findContours(image=img.copy(), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    hu_moments1 = []\n",
    "    hu_moments2 = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        moments = cv2.moments(contour)\n",
    "        hu = cv2.HuMoments(moments)\n",
    "        hu = np.array(hu).flatten()\n",
    "        d1 = np.linalg.norm(sd[0] - hu)\n",
    "        d2 = np.linalg.norm(sd[1] - hu)\n",
    "        hu_moments1.append(d1)\n",
    "        hu_moments2.append(d2)\n",
    "        # hu_moments.append(hu)\n",
    "\n",
    "    # moments = cv2.moments(np.array(hu_moments))\n",
    "    # hu = cv2.HuMoments(moments)\n",
    "\n",
    "    # return hu.flatten()\n",
    "    min_hu_moments = np.minimum(hu_moments1, hu_moments2)\n",
    "\n",
    "    if len(min_hu_moments) > 0:\n",
    "        max_hu = 1/min_hu_moments.sum()\n",
    "    else:\n",
    "        max_hu = 0\n",
    "\n",
    "    return [max_hu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c3ee519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing io module\n",
    "# import sys\n",
    "# sys.path.insert(1, \"./../io_utils/\")\n",
    "# sys.path.insert(1, \"./../preprocessing/\")\n",
    "# from io_utils import read_data, read_classes\n",
    "# from preprocessing import *\n",
    "# # reading data and class names\n",
    "# classes_names = read_classes('../ACdata_base/names.txt')\n",
    "# dataset_images, dataset_labels = read_data('../ACdata_base/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a959bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diacritics(d_img, bins=12):\n",
    "    return np.histogram(d_img.sum(axis=0), bins=bins)[0]/d_img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5b293ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Diacritics(d_img):\n",
    "#     \"\"\"Gets an input image of the diacritics image and return a score determines how those diacritics are similar to mohakek and thuluth\n",
    "        \n",
    "#     Args:\n",
    "#         diacritics_image: A binary image containing only diacritics (white text on black bg)\n",
    "#         recalculate: recalculate precalculate humoments values (default = False)\n",
    "#         path: path of image to recalculate humoments from (default = \"sds.png\")\n",
    "#         hu_file: file to read precalculated hu_moments from it or write to it in case of recalculate = True (default = \"sds_hue_moments.npy\")\n",
    "\n",
    "#     Returns:\n",
    "#         score: determine how those diacritics are similar to mohakek and thuluth (more similar => larger score)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # contours_d, _ = cv2.findContours(image=d_img, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "#     # contours_t, _ = cv2.findContours(image=t_img, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "#     # areas = []\n",
    "#     # for contour in contours:\n",
    "#     #     area = cv2.contourArea(contour)\n",
    "#     #     areas.append(area)\n",
    "\n",
    "#     # area_ratio = sum(areas)/(d_img.shape[0]*d_img.shape[1]) \n",
    "#     # return [d_img.sum()/img.sum()]    \n",
    "#     # return cv2.matchShapes(d_img, im, cv2.CONTOURS_MATCH_I1, 0)\n",
    "#     return cv2.HuMoments(cv2.moments(d_img)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f28d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690c320a",
   "metadata": {},
   "source": [
    "# 4. Words Orientation (WOr)\n",
    "One of the salient features of the Diwani style is the words written in a slanted format. WOr descriptor mainly specifies how, on average, words in text are oriented. the orientation of each word contour (without diacritics) is calculated and then a mean oriatation is calculated weighted by the area of each word contour.\n",
    "WOr algorithm was used to distinguish Diwani from other styles. Diwani style yieldan orientation average of about 45 degrees compared to 0 degrees by other styles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d0067101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def WOr(img, debug=False):\n",
    "#     \"\"\"Gets an input image of the text without diacritics and return the mean orientation of words\n",
    "        \n",
    "#     Args:\n",
    "#         text_only_image: A binary image containing only text wihtout diacritics (white text on black bg)\n",
    "#         debug: if true, show the bounding box of words (default False)\n",
    "#     Returns:\n",
    "#         orientation_mean: the weighted mean of words orientation\n",
    "#     \"\"\"\n",
    "\n",
    "#     contours, _ = cv2.findContours(image=img.copy(), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "#     orientations = []\n",
    "#     areas = []\n",
    "#     img = img.copy()\n",
    "#     if debug:\n",
    "#         rgb_img = cv2.cvtColor(img*255, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "#     # [vx,vy,x,y] = cv2.fitLine(contours[-1], cv2.DIST_L2,0,0.01,0.01)\n",
    "#     # return vx,vy,x,y\n",
    "#     # print(len(contours))\n",
    "#     for contour in contours:\n",
    "\n",
    "#         rows,cols = img.shape[:2]\n",
    "#         [vx,vy,x,y] = cv2.fitLine(contour, cv2.DIST_L2,0,0.01,0.01)\n",
    "#         # lefty = int((-x*vy/vx) + y)\n",
    "#         # righty = int(((cols-x)*vy/vx)+y)\n",
    "#         # cv2.line(img,(cols-1,righty),(0,lefty),(255,255,255),2)\n",
    "#         angle = np.rad2deg(np.arctan2(vy[0], vx[0]))\n",
    "#         area = cv2.contourArea(contour)\n",
    "\n",
    "#         orientations.append(angle)\n",
    "#         areas.append(area)\n",
    "#         continue\n",
    "#         rect = cv2.minAreaRect(contour)\n",
    "#         p, dimensions, orientation = rect\n",
    "#         # orientations.append(np.abs(orientation) % 60)\n",
    "#         orientations.append(np.abs(orientation))\n",
    "#         x,y,w,h = cv2.boundingRect(contour)\n",
    "#         # areas.append(h*w)\n",
    "#         areas.append(dimensions[0]*dimensions[1])\n",
    "#         # areas.append(w*h)\n",
    "#         # areas.append(w)\n",
    "        \n",
    "#         if debug:\n",
    "#             box = cv2.boxPoints(rect)\n",
    "#             box = np.int0(box)\n",
    "#             cv2.drawContours(rgb_img,[box],0,(255,255,255),1)\n",
    "    \n",
    "\n",
    "#     # # if debug:\n",
    "#     # plt.imshow(img, cmap=\"gray\")\n",
    "#     # return 0\n",
    "\n",
    "#     # sum_areas = np.sum(areas)\n",
    "#     # if sum_areas > 0:\n",
    "#     #     # weighted_mean = np.dot(areas, orientations) / sum_areas\n",
    "#     #     weighted_mean = orientations[np.argmax(areas)]\n",
    "#     # else:\n",
    "#     #     weighted_mean = 0\n",
    "\n",
    "\n",
    "#     m = 3\n",
    "#     n = len(areas)\n",
    "#     if len(areas) > m:\n",
    "#         n = m\n",
    "\n",
    "#     w = []\n",
    "\n",
    "#     if len(orientations) > 0:\n",
    "#         w.append(np.var(orientations))\n",
    "#         w.append(np.mean(orientations))\n",
    "#         # w.append(orientations[np.argmax(areas)])\n",
    "#     else:\n",
    "#         w.append(0)\n",
    "#         w.append(0)\n",
    "#         # w.append(0)\n",
    "\n",
    "#     max_n = np.argpartition(areas, -n)[-n:]\n",
    "#     for i in range(len(max_n)-1,-1,-1):\n",
    "#         w.append(orientations[i])\n",
    "\n",
    "#     for i in range(m-n):\n",
    "#         w.append(0)\n",
    "\n",
    "#     # return [weighted_mean]\n",
    "#     return w\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e3e683be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WOr(img):\n",
    "    \"\"\"Gets an input image of the text without diacritics and return the mean orientation of words\n",
    "        \n",
    "    Args:\n",
    "        text_only_image: A binary image containing only text wihtout diacritics (white text on black bg)\n",
    "        debug: if true, show the bounding box of words (default False)\n",
    "    Returns:\n",
    "        orientation_mean: the weighted mean of words orientation\n",
    "    \"\"\"\n",
    "    thetas = []\n",
    "    rhos = []\n",
    "    #img = morph_img\n",
    "    imgArea = img.shape[0] * img.shape[1]\n",
    "    # labeled, nr_objects = ndimage.label(img)\n",
    "    numLabels, _, stats, _ = cv2.connectedComponentsWithStats(img)\n",
    "    indexes = list(range(len(stats)))\n",
    "    # We sort the indeces list with heuristics being its key (Sorting heuristics and storing the indices of the sorted list)\n",
    "    indexes.sort(key=stats[:, cv2.CC_STAT_AREA].__getitem__, reverse=True)\n",
    "    # Then we map the actions_states list according to the indeces of the sorted list \n",
    "    stats = list(map(stats.__getitem__, indexes))\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        if i >= len(stats):\n",
    "            break\n",
    "        x = stats[i][cv2.CC_STAT_LEFT]\n",
    "        y = stats[i][cv2.CC_STAT_TOP]\n",
    "        w = stats[i][cv2.CC_STAT_WIDTH]\n",
    "        h = stats[i][cv2.CC_STAT_HEIGHT]\n",
    "        area = stats[i][cv2.CC_STAT_AREA]\n",
    "        if area < 0.001 * imgArea:\n",
    "            continue\n",
    "        bounded_image = img[y: y + h, x: x + w]\n",
    "        lines = cv2.HoughLines(bounded_image, 1, np.pi/180, 1)\n",
    "        if lines is not None:\n",
    "            line = lines[0][0]\n",
    "            rho = line[0]\n",
    "            theta = line[1]\n",
    "            if np.rad2deg(theta) > 60 or np.rad2deg(theta) < -10:\n",
    "                continue\n",
    "            rhos.append(rho)\n",
    "            thetas.append(theta)\n",
    "\n",
    "    contours, _ = cv2.findContours(img, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    orientations = []\n",
    "    areas = []\n",
    "    \n",
    "    for contour in contours:\n",
    "\n",
    "        rows,cols = img.shape[:2]\n",
    "        [vx,vy,x,y] = cv2.fitLine(contour, cv2.DIST_L2,0,0.01,0.01)\n",
    "        # lefty = int((-x*vy/vx) + y)\n",
    "        # righty = int(((cols-x)*vy/vx)+y)\n",
    "        # cv2.line(img,(cols-1,righty),(0,lefty),(255,255,255),2)\n",
    "        angle = np.rad2deg(np.arctan2(vy[0], vx[0]))\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        orientations.append(angle)\n",
    "        areas.append(area)\n",
    "    \n",
    "    m = 2\n",
    "    n = len(areas)\n",
    "    if len(areas) > m:\n",
    "        n = m\n",
    "\n",
    "    w = []\n",
    "\n",
    "    if len(orientations) > 0:\n",
    "        w.append(np.var(orientations))\n",
    "        w.append(np.mean(orientations))\n",
    "        # w.append(orientations[np.argmax(areas)])\n",
    "    else:\n",
    "        w.append(0)\n",
    "        w.append(0)\n",
    "        # w.append(0)\n",
    "\n",
    "    max_n = np.argpartition(areas, -n)[-n:]\n",
    "\n",
    "    for i in range(len(max_n)-1,-1,-1):\n",
    "        w.append(orientations[i])\n",
    "\n",
    "    for i in range(m-n):\n",
    "        w.append(0)\n",
    "    \n",
    "    # return [weighted_mean]\n",
    "    if len(thetas) == 0:\n",
    "        thetas.append(0)\n",
    "    if len(rhos) == 0:\n",
    "        rhos.append(0)\n",
    "\n",
    "    for i in range(len(rhos), 5):\n",
    "        rhos.append(0)\n",
    "    for i in range(len(thetas), 5):\n",
    "        thetas.append(0)\n",
    "\n",
    "    w.append(np.mean(rhos))\n",
    "    w.append(np.mean(thetas))\n",
    "\n",
    "    return np.append(rhos, np.append(thetas, w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "62f97818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HVSL(edge_image):\n",
    "    \"\"\"(Horizontal & Vertical Straight Lines Descriptor)\n",
    "    Gets the edge image as input and returns a vector of length 2 containing the features:\n",
    "        a- The appearance frequency of vertical and horizontal lines.\n",
    "        b- The ratio between the number of the pixels that constitute the texts' edges and the V/H lines' pixels.\n",
    "\n",
    "    Params:\n",
    "        edge_image: An image containing only the edges.\n",
    "        \n",
    "    Returns:\n",
    "        HVSL_features: a vector containing the two features mentioned above \n",
    "    \"\"\"\n",
    "\n",
    "    # A constant defining the minimum percentage of the row/column that a line should have\n",
    "    # The number of consecutive pixels divided by the total height/width should be at least this ratio to be considered a line\n",
    "    MINIMUM_VLINE_LENGTH_PERCENTAGE = 0.02\n",
    "    MINIMUM_HLINE_LENGTH_PERCENTAGE = 0.015\n",
    "\n",
    "    height, width = edge_image.shape\n",
    "    vertical_horizontal_lines_freq = 0\n",
    "    \n",
    "    vertical_size = max(int(MINIMUM_VLINE_LENGTH_PERCENTAGE * height), 1)\n",
    "    horizontal_size = max(int(MINIMUM_HLINE_LENGTH_PERCENTAGE * width), 1)\n",
    "    horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n",
    "    mask1 = cv2.morphologyEx(edge_image, cv2.MORPH_OPEN, horizontalStructure)\n",
    "    mask2 = cv2.morphologyEx(edge_image, cv2.MORPH_OPEN, verticalStructure)\n",
    "    \n",
    "    contours_h, _ = cv2.findContours(mask1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_v, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes_h = [cv2.boundingRect(contour) for contour in contours_h]\n",
    "    bounding_boxes_v = [cv2.boundingRect(contour) for contour in contours_v]\n",
    "    vertical_lines = [bounding_box[3] for bounding_box in bounding_boxes_v]\n",
    "    horizontal_lines = [bounding_box[2] for bounding_box in bounding_boxes_h]\n",
    "\n",
    "    vertical_horizontal_lines_pixels = sum(vertical_lines) + sum(horizontal_lines)\n",
    "    vertical_horizontal_lines_freq = len(vertical_lines) + len(horizontal_lines)\n",
    "\n",
    "    edge_pixels = edge_image.sum()\n",
    "\n",
    "    lines_edges_ratio = vertical_horizontal_lines_pixels / edge_pixels\n",
    "\n",
    "    HVSL_features = [vertical_horizontal_lines_freq, lines_edges_ratio]\n",
    "\n",
    "    return HVSL_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2eb43d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LVL(skeleton_image):\n",
    "    \"\"\"(Long Vertical Lines Descriptor)\n",
    "    Gets the skeleton image as input and returns a vector of length 5 containing the features:\n",
    "        a- The text height from the bottom to top.\n",
    "        b- The number of detected vertical lines.\n",
    "        c- The length of the highest detected vertical line.\n",
    "        d- The ratio between the text height and the highest vertical line.\n",
    "        e- The variance among the vertical lines.\n",
    "\n",
    "    Params:\n",
    "        skeleton_image: An image with the skeletonized version of the original input image.\n",
    "        \n",
    "    Returns:\n",
    "        LVL_features: a vector containing the five features mentioned above \n",
    "    \"\"\"\n",
    "    # TODO Crop the input image to remove the spaces between the text and the border\n",
    "\n",
    "    # A constant defining the minimum percentage of the column that a line should have\n",
    "    # The number of consecutive pixels divided by the total height should be at least this ratio to be considered a line\n",
    "    MINIMUM_VLINE_LENGTH_PERCENTAGE = 0.03\n",
    "\n",
    "    vertical_lines_freq = 0\n",
    "    height, width = skeleton_image.shape\n",
    "    vertical_lines = []\n",
    "    vertical_size = max(int(MINIMUM_VLINE_LENGTH_PERCENTAGE * height), 1)\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n",
    "\n",
    "    mask2 = cv2.morphologyEx(skeleton_image, cv2.MORPH_OPEN, verticalStructure)\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    vertical_lines = [bounding_box[3] for bounding_box in bounding_boxes]\n",
    "    vertical_lines_freq = len(bounding_boxes)\n",
    "\n",
    "    if vertical_lines_freq < 1:\n",
    "        highest_vertical_line_length = 0\n",
    "        vertical_lines_variance = 0\n",
    "    else:\n",
    "        highest_vertical_line_length = max(vertical_lines)\n",
    "        vertical_lines_variance = np.var(vertical_lines)\n",
    "\n",
    "    text_height = height\n",
    "    if text_height < 1:\n",
    "        highest_vertical_line_to_text_height_ratio = 0\n",
    "    else:   \n",
    "        highest_vertical_line_to_text_height_ratio = highest_vertical_line_length / text_height\n",
    "    \n",
    "\n",
    "    LVL = [text_height, vertical_lines_freq, highest_vertical_line_length, highest_vertical_line_to_text_height_ratio, vertical_lines_variance]\n",
    "    \n",
    "    return LVL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "623d12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToE_ToS(image, bins=10):\n",
    "    \"\"\"(Text orientation from Edges / Text orientation from Skeleton Descriptor)\n",
    "    Finds the orientation of edges / skeleton image by applying a sobel filter to capture the intensities of edges \n",
    "        and the orientation of these edges \n",
    "\n",
    "    Params:\n",
    "        image: An image with the skeletonized / edge version of the original input image.\n",
    "        \n",
    "    Returns:\n",
    "        ToE/ToS: a vector containing the intensities and orientations of the pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    Kx = np.array([[-2, 0, 2], \n",
    "                   [-1, 0, 1], \n",
    "                   [-2, 0, 2]], np.float32)\n",
    "\n",
    "    Ky = np.array([[2, 1, 2], \n",
    "                   [0, 0, 0], \n",
    "                   [-2, -1, -2]], np.float32)\n",
    "    \n",
    "    Ix = ndimage.filters.convolve(image, Kx)\n",
    "    Iy = ndimage.filters.convolve(image, Ky)\n",
    "    \n",
    "    G = np.hypot(Ix, Iy)\n",
    "    G = G / G.max()\n",
    "    # G = G[G != 0]\n",
    "    theta = np.arctan2(Iy, Ix)\n",
    "    # theta = theta[theta != 0]\n",
    "\n",
    "    Gs, _ = np.histogram(G, bins=bins)\n",
    "    # Gs = np.append(Gs, np.var(G))\n",
    "    thetas, _ = np.histogram(theta, bins=bins)\n",
    "    # thetas = np.append(thetas, np.var(theta))\n",
    "    G_sum = Gs.sum()\n",
    "    if G_sum > 1:\n",
    "        Gs = Gs / G_sum \n",
    "    T_sum = thetas.sum()\n",
    "    if T_sum > 1:\n",
    "        thetas = thetas / T_sum\n",
    "\n",
    "    return np.append(Gs, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b231e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPP(image, bins=10):\n",
    "    \"\"\"(Horizontal Profile Projection)\n",
    "    Finds the projection of the horizontal lines in an image and allocate them to specific bins based on their positions\n",
    "\n",
    "    Params:\n",
    "        image: A binary image\n",
    "        \n",
    "    Returns:\n",
    "        h: a vector containing the number of pixels in each bin \n",
    "    \"\"\"\n",
    "    white_pixels_in_row = image.sum(axis=1)\n",
    "\n",
    "    h, h_bins = np.histogram(white_pixels_in_row, range=(1, white_pixels_in_row.max()), density=False, bins=bins)\n",
    "\n",
    "    # normalizing the histogram\n",
    "    h_sum = h.sum()\n",
    "    if h_sum > 1:\n",
    "        h = h/h_sum\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bce5d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LBP(image):\n",
    "\n",
    "    lbp = local_binary_pattern(image, 6, 1.5, method='nri_uniform').flatten()\n",
    "\n",
    "    h, _ = np.histogram(lbp, bins=100)\n",
    "\n",
    "    # normalizing the histogram\n",
    "    h_sum = h.sum()\n",
    "    if h_sum > 1:\n",
    "        h = h/h_sum\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b977652",
   "metadata": {},
   "source": [
    "# 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ec37646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    # import randrange\n",
    "    from random import randrange\n",
    "    # importing io module\n",
    "    import sys\n",
    "    sys.path.insert(1, \"./../io_utils/\")\n",
    "    from io_utils import read_data, read_classes\n",
    "\n",
    "    # importing preprocessing module\n",
    "    sys.path.insert(1, \"./../preprocessing/\")\n",
    "    from preprocessing import binarize, extract_edges, extract_skeleton, separate_diacritics_and_text, crop\n",
    "\n",
    "    # reading data and class names\n",
    "    classes_names = read_classes('../ACdata_base/names.txt')\n",
    "    dataset_images, dataset_labels = read_data('../ACdata_base/')\n",
    "    \n",
    "    assert len(dataset_images) == len(dataset_labels)\n",
    "\n",
    "    # get the range of each class in the dataset\n",
    "    ranges = [0]\n",
    "    tmp = dataset_labels[0]\n",
    "    for idx, num in enumerate(dataset_labels):\n",
    "        if num != tmp:\n",
    "            tmp = num\n",
    "            ranges.append(idx)\n",
    "    ranges.append(len(dataset_labels))\n",
    "\n",
    "    # test_image = cv2.imread(\"../ACdata_base/7/1124.jpg\",0)\n",
    "    # diacritics_image, text_image = separate_diacritics_and_text(test_image)\n",
    "    # wor = WOr(text_image, True)\n",
    "    # print(wor)\n",
    "\n",
    "    # Choosing a random example from each class and apply the preprocessing Functions on it\n",
    "    for i, class_name in enumerate(classes_names):\n",
    "        # break\n",
    "        index = randrange(ranges[i], ranges[i+1])\n",
    "        test_image = dataset_images[index]\n",
    "        binary_image = binarize(test_image)\n",
    "        cropped_image = crop(binary_image)\n",
    "        skeleton_image = extract_skeleton(binary_image).astype(np.uint8)\n",
    "        edge_image = extract_edges(binary_image)\n",
    "        diacritics_image, text_image = separate_diacritics_and_text(binary_image)\n",
    "\n",
    "        assert len(np.unique(np.asarray(binary_image))) == 2\n",
    "\n",
    "        tth = Tth(skeleton_image, edge_image)\n",
    "        # sds = SDs(diacritics_image)[0]\n",
    "        # d_1, d_2 = Diacritics(diacritics_image, text_image)\n",
    "        # d = Diacritics(diacritics_image, cropped_image)\n",
    "        wor = WOr(text_image)[0]\n",
    "        hpp = HPP(cropped_image)\n",
    "        lvl = LVL(skeleton_image) #list of 5\n",
    "        hvsl = HVSL(edge_image) #list of 2\n",
    "        toe = ToE_ToS(edge_image,10) #list of 2\n",
    "        tos = ToE_ToS(skeleton_image,10) #list of 2\n",
    "\n",
    "        f, axarr = plt.subplots(1,1, figsize=(10, 7))\n",
    "\n",
    "        suptitle = \"Tth: \" + np.array2string(tth)\n",
    "        # suptitle += \"\\nSDs: \" + str(sds)\n",
    "        # suptitle += \"\\nSDs: \" + str(d_1) + \" \" + str(d_2) + \" \" + str(d_1/d_2)\n",
    "        # suptitle += \"\\nSDs: \" + str(d)\n",
    "        suptitle += \"\\nWOr: \" + str(wor)\n",
    "        suptitle += \"\\nToE: \" + np.array2string(toe)\n",
    "        suptitle += \"\\nToS: \" + np.array2string(tos)\n",
    "        suptitle += \"\\nHPP: \" + str(hpp)\n",
    "        suptitle += \"\\nHVSL: \" + str(hvsl)\n",
    "        suptitle += \"\\nLVL: \" + str(lvl)\n",
    "        f.suptitle(suptitle)\n",
    "\n",
    "        axarr.imshow(cropped_image, cmap='gray')\n",
    "        axarr.set_title(class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "766aa718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "50bafc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py():\n",
    "    !jupyter nbconvert --to script feature_extraction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3d7d6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook feature_extraction.ipynb to script\n",
      "[NbConvertApp] Writing 24472 bytes to feature_extraction.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3609144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
