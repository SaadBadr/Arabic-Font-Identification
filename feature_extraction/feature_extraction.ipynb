{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d1b7d9",
   "metadata": {},
   "source": [
    "Hello everyone,\n",
    "\n",
    "In this module, it is required to implement the feature extractor for Arabic Font Identification System.\n",
    "\n",
    "At first, what are the main steps that we should go through in this module?\n",
    "\n",
    "\n",
    "# TODOs:\n",
    "\n",
    "1. Understand the problem\n",
    "2. ..\n",
    "3. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbcf51",
   "metadata": {},
   "source": [
    "# 1. Understand the problem\n",
    "\n",
    "Calligraphers use some specific features, The main aim of this step is to transform these features into values that can be fed to a machine so it can decide on the style of a given text image. It should be mentioned that a feature might specify one or more styles. Features designated for each style or set of styles might be used in a sequential manner (sequential\n",
    "decision) or parallel manner.\n",
    "\n",
    "\n",
    "\n",
    "- Input: --\n",
    "- Output: --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b311bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### imports #####################################################\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb1f9a",
   "metadata": {},
   "source": [
    "# 2. Text thickness (Tth)\n",
    "Stroke thickness plays an important role in defining the style. Some styles use a flat pen, whereas some others use a pointed one. In some styles, calligraphers alter the thickness while writing (via pushing down the pen or the opposite), whereas in others, the thickness is always preserved. Modeling such a feature in form of a descriptor will help the machine to understand more specificities of each style. Text thickness (Tth) descriptor codifies the appearance frequency of different line thicknesses in a text image. To extract this descriptor, we employ both the skeleton and edge image, and thickness is determined by the distance between skeleton and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae7eccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tth(skeleton_img, edge_img, bins=5):\n",
    "    \"\"\"Gets normalized histogram of text thickness \n",
    "\n",
    "    Args:\n",
    "        skeleton_img: the image extracted skeleton\n",
    "        edge_img: the image extracted edges\n",
    "        \n",
    "    Returns:\n",
    "        hist : 1d array contains the normalized histogram values\n",
    "        bin_edges : 1d array contains the bin edges values\n",
    "    \"\"\"\n",
    "\n",
    "    skeleton = skeleton_img.copy()\n",
    "    edge = edge_img.copy()\n",
    "\n",
    "    # convert img to uint8 with 255:white 0:black\n",
    "\n",
    "    if skeleton.max() == 1:\n",
    "        skeleton = skeleton * 255\n",
    "        skeleton = skeleton.astype(np.uint8)\n",
    "\n",
    "    if edge.max() == 1:\n",
    "        edge = edge * 255\n",
    "        edge = edge.astype(np.uint8)\n",
    "\n",
    "    # flipping edge => black text on white background\n",
    "    edge = 255 - edge\n",
    "\n",
    "    distance = cv2.distanceTransform(edge+skeleton, distanceType=cv2.DIST_L2, maskSize=5)\n",
    "    # keeping only skeleton distances\n",
    "    distance[skeleton==0] = 0\n",
    "    \n",
    "    h, h_bins = np.histogram(distance, range=(1, distance.max()), density=False, bins=bins)\n",
    "\n",
    "    # normalizing the histogram\n",
    "    h_sum = h.sum()\n",
    "    if h_sum > 1:\n",
    "        h = h/h_sum\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47249f",
   "metadata": {},
   "source": [
    "# 3. Diacritics\n",
    "Thuluth and Mohakik have a similar writing style that is decorated with diacritics\n",
    "A Diacritics descriptor will be used to inspect the density of diacritics in a given text image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a959bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diacritics(d_img, bins=12):\n",
    "    return np.histogram(d_img.sum(axis=0), bins=bins)[0]/d_img.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c320a",
   "metadata": {},
   "source": [
    "# 4. Words Orientation (WOr)\n",
    "One of the salient features of the Diwani style is the words written in a slanted format. WOr descriptor mainly specifies how, on average, words in text are oriented. the orientation of each word contour (without diacritics) is calculated and then a mean oriatation is calculated weighted by the area of each word contour.\n",
    "WOr algorithm was used to distinguish Diwani from other styles. Diwani style yieldan orientation average of about 45 degrees compared to 0 degrees by other styles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e683be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WOr(img):\n",
    "    \"\"\"Gets an input image of the text without diacritics and return the mean orientation of words\n",
    "        \n",
    "    Args:\n",
    "        text_only_image: A binary image containing only text wihtout diacritics (white text on black bg)\n",
    "        debug: if true, show the bounding box of words (default False)\n",
    "    Returns:\n",
    "        orientation_mean: the weighted mean of words orientation\n",
    "    \"\"\"\n",
    "    thetas = []\n",
    "    rhos = []\n",
    "    imgArea = img.shape[0] * img.shape[1]\n",
    "    numLabels, _, stats, _ = cv2.connectedComponentsWithStats(img)\n",
    "    indexes = list(range(len(stats)))\n",
    "    # We sort the indeces list with heuristics being its key (Sorting heuristics and storing the indices of the sorted list)\n",
    "    indexes.sort(key=stats[:, cv2.CC_STAT_AREA].__getitem__, reverse=True)\n",
    "    # Then we map the actions_states list according to the indeces of the sorted list \n",
    "    stats = list(map(stats.__getitem__, indexes))\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        if i >= len(stats):\n",
    "            break\n",
    "        x = stats[i][cv2.CC_STAT_LEFT]\n",
    "        y = stats[i][cv2.CC_STAT_TOP]\n",
    "        w = stats[i][cv2.CC_STAT_WIDTH]\n",
    "        h = stats[i][cv2.CC_STAT_HEIGHT]\n",
    "        area = stats[i][cv2.CC_STAT_AREA]\n",
    "        if area < 0.001 * imgArea:\n",
    "            continue\n",
    "        bounded_image = img[y: y + h, x: x + w]\n",
    "        lines = cv2.HoughLines(bounded_image, 1, np.pi/180, 1)\n",
    "        if lines is not None:\n",
    "            line = lines[0][0]\n",
    "            rho = line[0]\n",
    "            theta = line[1]\n",
    "            if np.rad2deg(theta) > 60 or np.rad2deg(theta) < -10:\n",
    "                continue\n",
    "            rhos.append(rho)\n",
    "            thetas.append(theta)\n",
    "\n",
    "    contours, _ = cv2.findContours(img, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    orientations = []\n",
    "    areas = []\n",
    "    \n",
    "    for contour in contours:\n",
    "\n",
    "        rows,cols = img.shape[:2]\n",
    "        [vx,vy,x,y] = cv2.fitLine(contour, cv2.DIST_L2,0,0.01,0.01)\n",
    "        angle = np.rad2deg(np.arctan2(vy[0], vx[0]))\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        orientations.append(angle)\n",
    "        areas.append(area)\n",
    "    \n",
    "    m = 2\n",
    "    n = len(areas)\n",
    "    if len(areas) > m:\n",
    "        n = m\n",
    "\n",
    "    w = []\n",
    "\n",
    "    if len(orientations) > 0:\n",
    "        w.append(np.var(orientations))\n",
    "        w.append(np.mean(orientations))\n",
    "\n",
    "    else:\n",
    "        w.append(0)\n",
    "        w.append(0)\n",
    "\n",
    "    max_n = np.argpartition(areas, -n)[-n:]\n",
    "\n",
    "    for i in range(len(max_n)-1,-1,-1):\n",
    "        w.append(orientations[i])\n",
    "\n",
    "    for i in range(m-n):\n",
    "        w.append(0)\n",
    "    \n",
    "\n",
    "    if len(thetas) == 0:\n",
    "        thetas.append(0)\n",
    "    if len(rhos) == 0:\n",
    "        rhos.append(0)\n",
    "\n",
    "    for i in range(len(rhos), 5):\n",
    "        rhos.append(0)\n",
    "    for i in range(len(thetas), 5):\n",
    "        thetas.append(0)\n",
    "\n",
    "    w.append(np.mean(rhos))\n",
    "    w.append(np.mean(thetas))\n",
    "\n",
    "    return np.append(rhos, np.append(thetas, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f97818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HVSL(edge_image):\n",
    "    \"\"\"(Horizontal & Vertical Straight Lines Descriptor)\n",
    "    Gets the edge image as input and returns a vector of length 2 containing the features:\n",
    "        a- The appearance frequency of vertical and horizontal lines.\n",
    "        b- The ratio between the number of the pixels that constitute the texts' edges and the V/H lines' pixels.\n",
    "\n",
    "    Params:\n",
    "        edge_image: An image containing only the edges.\n",
    "        \n",
    "    Returns:\n",
    "        HVSL_features: a vector containing the two features mentioned above \n",
    "    \"\"\"\n",
    "\n",
    "    # A constant defining the minimum percentage of the row/column that a line should have\n",
    "    # The number of consecutive pixels divided by the total height/width should be at least this ratio to be considered a line\n",
    "    MINIMUM_VLINE_LENGTH_PERCENTAGE = 0.02\n",
    "    MINIMUM_HLINE_LENGTH_PERCENTAGE = 0.015\n",
    "\n",
    "    height, width = edge_image.shape\n",
    "    vertical_horizontal_lines_freq = 0\n",
    "    \n",
    "    vertical_size = max(int(MINIMUM_VLINE_LENGTH_PERCENTAGE * height), 1)\n",
    "    horizontal_size = max(int(MINIMUM_HLINE_LENGTH_PERCENTAGE * width), 1)\n",
    "    horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n",
    "    mask1 = cv2.morphologyEx(edge_image, cv2.MORPH_OPEN, horizontalStructure)\n",
    "    mask2 = cv2.morphologyEx(edge_image, cv2.MORPH_OPEN, verticalStructure)\n",
    "    \n",
    "    contours_h, _ = cv2.findContours(mask1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_v, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes_h = [cv2.boundingRect(contour) for contour in contours_h]\n",
    "    bounding_boxes_v = [cv2.boundingRect(contour) for contour in contours_v]\n",
    "    vertical_lines = [bounding_box[3] for bounding_box in bounding_boxes_v]\n",
    "    horizontal_lines = [bounding_box[2] for bounding_box in bounding_boxes_h]\n",
    "\n",
    "    vertical_horizontal_lines_pixels = sum(vertical_lines) + sum(horizontal_lines)\n",
    "    vertical_horizontal_lines_freq = len(vertical_lines) + len(horizontal_lines)\n",
    "\n",
    "    edge_pixels = edge_image.sum()\n",
    "\n",
    "    lines_edges_ratio = vertical_horizontal_lines_pixels / edge_pixels\n",
    "\n",
    "    HVSL_features = [vertical_horizontal_lines_freq, lines_edges_ratio]\n",
    "\n",
    "    return HVSL_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb43d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LVL(skeleton_image):\n",
    "    \"\"\"(Long Vertical Lines Descriptor)\n",
    "    Gets the skeleton image as input and returns a vector of length 5 containing the features:\n",
    "        a- The text height from the bottom to top.\n",
    "        b- The number of detected vertical lines.\n",
    "        c- The length of the highest detected vertical line.\n",
    "        d- The ratio between the text height and the highest vertical line.\n",
    "        e- The variance among the vertical lines.\n",
    "\n",
    "    Params:\n",
    "        skeleton_image: An image with the skeletonized version of the original input image.\n",
    "        \n",
    "    Returns:\n",
    "        LVL_features: a vector containing the five features mentioned above \n",
    "    \"\"\"\n",
    "    # TODO Crop the input image to remove the spaces between the text and the border\n",
    "\n",
    "    # A constant defining the minimum percentage of the column that a line should have\n",
    "    # The number of consecutive pixels divided by the total height should be at least this ratio to be considered a line\n",
    "    MINIMUM_VLINE_LENGTH_PERCENTAGE = 0.03\n",
    "\n",
    "    vertical_lines_freq = 0\n",
    "    height, width = skeleton_image.shape\n",
    "    vertical_lines = []\n",
    "    vertical_size = max(int(MINIMUM_VLINE_LENGTH_PERCENTAGE * height), 1)\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vertical_size))\n",
    "\n",
    "    mask2 = cv2.morphologyEx(skeleton_image, cv2.MORPH_OPEN, verticalStructure)\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    vertical_lines = [bounding_box[3] for bounding_box in bounding_boxes]\n",
    "    vertical_lines_freq = len(bounding_boxes)\n",
    "\n",
    "    if vertical_lines_freq < 1:\n",
    "        highest_vertical_line_length = 0\n",
    "        vertical_lines_variance = 0\n",
    "    else:\n",
    "        highest_vertical_line_length = max(vertical_lines)\n",
    "        vertical_lines_variance = np.var(vertical_lines)\n",
    "\n",
    "    text_height = height\n",
    "    if text_height < 1:\n",
    "        highest_vertical_line_to_text_height_ratio = 0\n",
    "    else:   \n",
    "        highest_vertical_line_to_text_height_ratio = highest_vertical_line_length / text_height\n",
    "    \n",
    "\n",
    "    LVL = [text_height, vertical_lines_freq, highest_vertical_line_length, highest_vertical_line_to_text_height_ratio, vertical_lines_variance]\n",
    "    \n",
    "    return LVL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "623d12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToE_ToS(image, bins=10):\n",
    "    \"\"\"(Text orientation from Edges / Text orientation from Skeleton Descriptor)\n",
    "    Finds the orientation of edges / skeleton image by applying a sobel filter to capture the intensities of edges \n",
    "        and the orientation of these edges \n",
    "\n",
    "    Params:\n",
    "        image: An image with the skeletonized / edge version of the original input image.\n",
    "        \n",
    "    Returns:\n",
    "        ToE/ToS: a vector containing the intensities and orientations of the pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    Kx = np.array([[-2, 0, 2], \n",
    "                   [-1, 0, 1], \n",
    "                   [-2, 0, 2]], np.float32)\n",
    "\n",
    "    Ky = np.array([[2, 1, 2], \n",
    "                   [0, 0, 0], \n",
    "                   [-2, -1, -2]], np.float32)\n",
    "    \n",
    "    Ix = ndimage.filters.convolve(image, Kx)\n",
    "    Iy = ndimage.filters.convolve(image, Ky)\n",
    "    \n",
    "    G = np.hypot(Ix, Iy)\n",
    "    G = G / G.max()\n",
    "    # G = G[G != 0]\n",
    "    theta = np.arctan2(Iy, Ix)\n",
    "    # theta = theta[theta != 0]\n",
    "\n",
    "    Gs, _ = np.histogram(G, bins=bins)\n",
    "    # Gs = np.append(Gs, np.var(G))\n",
    "    thetas, _ = np.histogram(theta, bins=bins)\n",
    "    # thetas = np.append(thetas, np.var(theta))\n",
    "    G_sum = Gs.sum()\n",
    "    if G_sum > 1:\n",
    "        Gs = Gs / G_sum \n",
    "    T_sum = thetas.sum()\n",
    "    if T_sum > 1:\n",
    "        thetas = thetas / T_sum\n",
    "\n",
    "    return np.append(Gs, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b231e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPP(image, bins=10):\n",
    "    \"\"\"(Horizontal Profile Projection)\n",
    "    Finds the projection of the horizontal lines in an image and allocate them to specific bins based on their positions\n",
    "\n",
    "    Params:\n",
    "        image: A binary image\n",
    "        \n",
    "    Returns:\n",
    "        h: a vector containing the number of pixels in each bin \n",
    "    \"\"\"\n",
    "    white_pixels_in_row = image.sum(axis=1)\n",
    "\n",
    "    h, h_bins = np.histogram(white_pixels_in_row, range=(1, white_pixels_in_row.max()), density=False, bins=bins)\n",
    "\n",
    "    # normalizing the histogram\n",
    "    h_sum = h.sum()\n",
    "    if h_sum > 1:\n",
    "        h = h/h_sum\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bce5d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPQ(img,winSize=5,freqestim=2,hist_size=1024):\n",
    "\n",
    "    STFTalpha=1/winSize  # alpha in STFT approaches (for Gaussian derivative alpha=1)\n",
    "    sigmaS=(winSize-1)/4 # Sigma for STFT Gaussian window (applied if freqestim==2)\n",
    "\n",
    "    convmode='valid' # Compute descriptor responses only on part that have full neigborhood. Use 'same' if all pixels are included (extrapolates np.image with zeros).\n",
    "\n",
    "    img=np.float64(img) # Convert np.image to double\n",
    "    r=(winSize-1)/2 # Get radius from window size\n",
    "    x=np.arange(-r,r+1)[np.newaxis] # Form spatial coordinates in window\n",
    "    u=np.arange(1,r+1)[np.newaxis]\n",
    "\n",
    "    if freqestim==1:  #  STFT uniform window\n",
    "        #  Basic STFT filters\n",
    "        w0=np.ones_like(x)\n",
    "        w1=np.exp(-2*np.pi*x*STFTalpha*1j)\n",
    "            # exp(complex(0,-2*pi*x*STFTalpha))\n",
    "        w2=np.conj(w1)\n",
    "\n",
    "    elif freqestim == 2:\n",
    "        w0=(x*0+1)\n",
    "        w1=np.exp(-2*np.pi*x*STFTalpha*1j)\n",
    "        w2=np.conj(w1)\n",
    "\n",
    "        # Gaussian window\n",
    "        gs=np.exp(-0.5*np.power(x / sigmaS, 2)) / (np.sqrt(2*np.pi) * sigmaS)\n",
    "\n",
    "        # Windowed filters\n",
    "        w0=gs * w0\n",
    "        w1=gs * w1\n",
    "        w2=gs * w2\n",
    "\n",
    "        # Normalize to zero mean \n",
    "        w1=w1-np.mean(w1)\n",
    "        w2=w2-np.mean(w2)\n",
    "\n",
    "    ## Run filters to compute the frequency response in the four points. Store np.real and np.imaginary parts separately\n",
    "    # Run first filter\n",
    "\n",
    "    filterResp1=convolve2d(convolve2d(img,w0.T,convmode),w1,convmode)\n",
    "    filterResp2=convolve2d(convolve2d(img,w1.T,convmode),w0,convmode)\n",
    "    filterResp3=convolve2d(convolve2d(img,w1.T,convmode),w1,convmode)\n",
    "    filterResp4=convolve2d(convolve2d(img,w1.T,convmode),w2,convmode)\n",
    "\n",
    "        # Initilize frequency domain matrix for four frequency coordinates (np.real and np.imaginary parts for each frequency).\n",
    "    freqResp=np.dstack([filterResp1.real, filterResp1.imag,\n",
    "                        filterResp2.real, filterResp2.imag,\n",
    "                        filterResp3.real, filterResp3.imag,\n",
    "                        filterResp4.real, filterResp4.imag])\n",
    "\n",
    "    ## Perform quantization and compute LPQ codewords\n",
    "    inds = np.arange(freqResp.shape[2])[np.newaxis,np.newaxis,:]\n",
    "    LPQdesc=((freqResp>0)*(2**inds)).sum(2)\n",
    "\n",
    "    LPQdesc=np.histogram(LPQdesc.flatten(),range(hist_size))[0]\n",
    "\n",
    "    LPQdesc=LPQdesc/LPQdesc.sum()\n",
    "    \n",
    "    return LPQdesc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b977652",
   "metadata": {},
   "source": [
    "# 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec37646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    # import randrange\n",
    "    from random import randrange\n",
    "    # importing io module\n",
    "    import sys\n",
    "    sys.path.insert(1, \"./../io_utils/\")\n",
    "    from io_utils import read_data, read_classes\n",
    "\n",
    "    # importing preprocessing module\n",
    "    sys.path.insert(1, \"./../preprocessing/\")\n",
    "    from preprocessing import binarize, extract_edges, extract_skeleton, separate_diacritics_and_text, crop\n",
    "\n",
    "    # reading data and class names\n",
    "    classes_names = read_classes('../ACdata_base/names.txt')\n",
    "    dataset_images, dataset_labels = read_data('../ACdata_base/')\n",
    "    \n",
    "    assert len(dataset_images) == len(dataset_labels)\n",
    "\n",
    "    # get the range of each class in the dataset\n",
    "    ranges = [0]\n",
    "    tmp = dataset_labels[0]\n",
    "    for idx, num in enumerate(dataset_labels):\n",
    "        if num != tmp:\n",
    "            tmp = num\n",
    "            ranges.append(idx)\n",
    "    ranges.append(len(dataset_labels))\n",
    "\n",
    "    # test_image = cv2.imread(\"../ACdata_base/7/1124.jpg\",0)\n",
    "    # diacritics_image, text_image = separate_diacritics_and_text(test_image)\n",
    "    # wor = WOr(text_image, True)\n",
    "    # print(wor)\n",
    "\n",
    "    # Choosing a random example from each class and apply the preprocessing Functions on it\n",
    "    for i, class_name in enumerate(classes_names):\n",
    "        # break\n",
    "        index = randrange(ranges[i], ranges[i+1])\n",
    "        test_image = dataset_images[index]\n",
    "        binary_image = binarize(test_image)\n",
    "        cropped_image = crop(binary_image)\n",
    "        skeleton_image = extract_skeleton(binary_image).astype(np.uint8)\n",
    "        edge_image = extract_edges(binary_image)\n",
    "        diacritics_image, text_image = separate_diacritics_and_text(binary_image)\n",
    "\n",
    "        assert len(np.unique(np.asarray(binary_image))) == 2\n",
    "\n",
    "        tth = Tth(skeleton_image, edge_image)\n",
    "        diacritics = Diacritics(diacritics_image)\n",
    "        wor = WOr(text_image)[0]\n",
    "        hpp = HPP(cropped_image)\n",
    "        lvl = LVL(skeleton_image) #list of 5\n",
    "        hvsl = HVSL(edge_image) #list of 2\n",
    "        toe = ToE_ToS(edge_image,10) #list of 2\n",
    "        tos = ToE_ToS(skeleton_image,10) #list of 2\n",
    "        lpq = LPQ(cropped_image)\n",
    "\n",
    "        f, axarr = plt.subplots(1,1, figsize=(10, 7))\n",
    "\n",
    "        suptitle = \"Tth: \" + np.array2string(tth)\n",
    "        suptitle += \"\\nWOr: \" + str(wor)\n",
    "        suptitle += \"\\nToE: \" + np.array2string(toe)\n",
    "        suptitle += \"\\nToS: \" + np.array2string(tos)\n",
    "        suptitle += \"\\nHPP: \" + str(hpp)\n",
    "        suptitle += \"\\nHVSL: \" + str(hvsl)\n",
    "        suptitle += \"\\nLVL: \" + str(lvl)\n",
    "        f.suptitle(suptitle)\n",
    "\n",
    "        axarr.imshow(cropped_image, cmap='gray')\n",
    "        axarr.set_title(class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "766aa718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50bafc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py():\n",
    "    !jupyter nbconvert --to script feature_extraction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d7d6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook feature_extraction.ipynb to script\n",
      "[NbConvertApp] Writing 18045 bytes to feature_extraction.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3609144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
